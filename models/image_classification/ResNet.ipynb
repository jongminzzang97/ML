{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "논문을 참고하여 ResNet을 구현해보려하고 한다.  \n",
    "부스트캠프 내에서 한번 해봤는데 그 때 도움이 많이 되었다고 느껴 다시 한번 해보려고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet은 가중치를 잃지 않기 위한 방법으로 Residual Learning을 이용하여 성능을 매우 향상시켰으며  \n",
    "이 이후로 등장하는 네트워크들은 이와 비슷한 시도를 많이 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResidualBlock은 다음 그림을 나타내며 ResNet을 구현할 때 하나의 단위로서 이용된다.  \n",
    "  \n",
    "![ResidualBlock](https://miro.medium.com/max/1140/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 단위 블락\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, strides):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=strides[0], padding=1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU()\n",
    "                        )\n",
    "                    \n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=strides[1], padding=1),\n",
    "                        nn.BatchNorm2d(out_channels)\n",
    "                        ) \n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=2)\n",
    "        else:\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = out + self.shortcut(x)\n",
    "        out = nn.functional.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "논문의 이미지를 보고 직관적으로 이해할 수 있도록 구현하였다.  \n",
    "위에서 구현한 ResidualBlock을 이용한다.\n",
    "\n",
    "\n",
    "![ResidualBlock](https://pytorch.kr/assets/images/resnet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.num_class= num_class\n",
    "        self.layer = nn.Sequential(\n",
    "\n",
    "            # conv1 \n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=1),\n",
    "\n",
    "            # conv2_x\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            ResidualBlock(in_channels=64, out_channels=64, strides=(1, 1)),\n",
    "            ResidualBlock(in_channels=64, out_channels=64, strides=(1, 1)),\n",
    "            ResidualBlock(in_channels=64, out_channels=64, strides=(1, 1)),\n",
    "            \n",
    "\n",
    "            # conv3_x\n",
    "            ResidualBlock(in_channels=64, out_channels=128, strides=(2, 1)),\n",
    "            ResidualBlock(in_channels=128, out_channels=128, strides=(1, 1)),\n",
    "            ResidualBlock(in_channels=128, out_channels=128, strides=(1, 1)),\n",
    "            ResidualBlock(in_channels=128, out_channels=128, strides=(1, 1)),\n",
    "\n",
    "            # conv4_x\n",
    "            ResidualBlock(in_channels=128, out_channels=256, strides=(2, 1)),\n",
    "            ResidualBlock(in_channels=256, out_channels=256, strides=(1, 1)),\n",
    "            ResidualBlock(in_channels=256, out_channels=256, strides=(1, 1)),\n",
    "            ResidualBlock(in_channels=256, out_channels=256, strides=(1, 1)),\n",
    "            ResidualBlock(in_channels=256, out_channels=256, strides=(1, 1)),\n",
    "            ResidualBlock(in_channels=256, out_channels=256, strides=(1, 1)),\n",
    "\n",
    "            # conv5_x\n",
    "            ResidualBlock(in_channels=256, out_channels=512, strides=(2, 1)),\n",
    "            ResidualBlock(in_channels=512, out_channels=512, strides=(1, 1)),\n",
    "            ResidualBlock(in_channels=512, out_channels=512, strides=(1, 1)),\n",
    "\n",
    "\n",
    "            # 1x1 average pool, 1000-d fc, softmax\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, num_class),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델이 제대로 만들어 졌는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3172, 0.0524, 0.0309, 0.0886, 0.1584, 0.0903, 0.0578, 0.0435, 0.0901,\n",
      "         0.0708]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jongmin\\miniconda3\\envs\\open-mmlab\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(1, 3, 227, 227)\n",
    "\n",
    "# output channel을 10으로 설정해봄\n",
    "resnet = ResNet34(10)\n",
    "print(resnet(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 갖고 있는 데이터로 학습해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만든 모델을 가지고 있는 데이터에 대해서 학습해보자.  \n",
    "나는 음식점에서 찍은 실내, 실외, 음식 3가지의 분류의 데이터를 이용해서 학습한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31500\n",
      "13499\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transforms = T.Compose([\n",
    "                        T.Resize((224,224)),\n",
    "                        T.ToTensor()\n",
    "                    ])\n",
    "\n",
    "train_dataset = ImageFolder(root='./images/images_train', transform=transforms)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = ImageFolder(root='./images/images_test',transform=transforms)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 진행\n",
    "1. model 정의 \n",
    "2. loss 함수 정의\n",
    "3. optimizer 정의\n",
    "4. scheduler 정의\n",
    "\n",
    "일반적으로 위와 같은 내용을 정의하고 학습을 진행하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "model = ResNet34(3).to(device)\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[28,32], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/493 [00:00<?, ?it/s]C:\\Users\\jongmin\\miniconda3\\envs\\open-mmlab\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "100%|██████████| 493/493 [03:37<00:00,  2.27it/s]\n",
      "100%|██████████| 211/211 [00:45<00:00,  4.62it/s]\n",
      "100%|██████████| 493/493 [03:09<00:00,  2.60it/s]\n",
      "100%|██████████| 493/493 [03:05<00:00,  2.66it/s]\n",
      "100%|██████████| 493/493 [03:11<00:00,  2.57it/s]\n",
      "100%|██████████| 493/493 [03:05<00:00,  2.66it/s]\n",
      "100%|██████████| 493/493 [02:53<00:00,  2.84it/s]\n",
      "100%|██████████| 211/211 [00:36<00:00,  5.86it/s]\n",
      "100%|██████████| 493/493 [02:53<00:00,  2.84it/s]\n",
      "100%|██████████| 493/493 [02:53<00:00,  2.84it/s]\n",
      "100%|██████████| 493/493 [03:05<00:00,  2.66it/s]\n",
      "100%|██████████| 493/493 [03:15<00:00,  2.52it/s]\n",
      "100%|██████████| 493/493 [03:16<00:00,  2.50it/s]\n",
      "100%|██████████| 211/211 [00:40<00:00,  5.21it/s]\n",
      "100%|██████████| 493/493 [03:17<00:00,  2.49it/s]\n",
      "100%|██████████| 493/493 [03:09<00:00,  2.60it/s]\n",
      "100%|██████████| 493/493 [03:10<00:00,  2.59it/s]\n",
      "100%|██████████| 493/493 [03:08<00:00,  2.61it/s]\n",
      "100%|██████████| 493/493 [03:06<00:00,  2.65it/s]\n",
      "100%|██████████| 211/211 [00:38<00:00,  5.41it/s]\n",
      "100%|██████████| 493/493 [03:06<00:00,  2.64it/s]\n",
      "100%|██████████| 493/493 [03:06<00:00,  2.64it/s]\n",
      "100%|██████████| 493/493 [03:06<00:00,  2.64it/s]\n",
      "100%|██████████| 493/493 [03:06<00:00,  2.64it/s]\n",
      "100%|██████████| 493/493 [03:06<00:00,  2.64it/s]\n",
      "100%|██████████| 211/211 [00:39<00:00,  5.37it/s]\n",
      "100%|██████████| 493/493 [03:13<00:00,  2.55it/s]\n",
      "100%|██████████| 493/493 [02:58<00:00,  2.76it/s]\n",
      "100%|██████████| 493/493 [02:54<00:00,  2.82it/s]\n",
      "100%|██████████| 493/493 [02:54<00:00,  2.82it/s]\n",
      "100%|██████████| 493/493 [02:54<00:00,  2.82it/s]\n",
      "100%|██████████| 211/211 [00:36<00:00,  5.81it/s]\n",
      "100%|██████████| 493/493 [02:54<00:00,  2.82it/s]\n",
      "100%|██████████| 493/493 [02:54<00:00,  2.82it/s]\n",
      "100%|██████████| 493/493 [02:54<00:00,  2.82it/s]\n",
      "100%|██████████| 493/493 [03:02<00:00,  2.71it/s]\n",
      "100%|██████████| 493/493 [03:08<00:00,  2.61it/s]\n",
      "100%|██████████| 211/211 [00:39<00:00,  5.35it/s]\n",
      "100%|██████████| 493/493 [03:07<00:00,  2.63it/s]\n",
      "100%|██████████| 493/493 [03:10<00:00,  2.59it/s]\n",
      "100%|██████████| 493/493 [03:08<00:00,  2.61it/s]\n",
      "100%|██████████| 493/493 [03:08<00:00,  2.62it/s]\n",
      "100%|██████████| 211/211 [00:38<00:00,  5.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "with open('train.log', 'w') as log:\n",
    "    for Epoch in range(35):\n",
    "\n",
    "        # var for print loss and acc\n",
    "        epoch_loss = 0.\n",
    "        epoch_corrects = 0\n",
    "        model.train()\n",
    "        for i, (img, target) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 모델에 이미지를 집어넣고 결과를 받음\n",
    "\n",
    "            out = model(img)\n",
    "\n",
    "            # loss를 구하고 optimizer를 이용하여 기울기 변화\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_corrects += torch.sum(torch.argmax(out, dim=1) == target.data)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_loss = epoch_loss/len(train_dataloader)\n",
    "        epoch_acc = epoch_corrects/len(train_dataset)\n",
    "        \n",
    "\n",
    "        # validataion\n",
    "        val_loss = 0.\n",
    "        val_acc = 0.\n",
    "        val_corrects = 0\n",
    "        if Epoch % 5 == 0 or Epoch == 34:\n",
    "            model.eval()\n",
    "            for i, (img, target) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "                img, target = img.to(device), target.to(device)\n",
    "                with torch.no_grad():\n",
    "                    out = model(img)\n",
    "                loss = criterion(out, target)\n",
    "\n",
    "                val_loss += loss\n",
    "                val_corrects += torch.sum(torch.argmax(out, dim=1) == target.data)\n",
    "\n",
    "            val_loss = val_loss/len(test_dataloader)\n",
    "            val_acc = val_corrects/len(test_dataset)\n",
    "        log.write('Epoch %d ||| train_loss: %.4f, train_acc: %.4f, valid_loss: %.4f, valid_acc: %.4f\\n'%(Epoch, epoch_loss, epoch_acc, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-mmlab",
   "language": "python",
   "name": "open-mmlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "12c0982b496adaafbf3cfd9dd6a6f1d65f5fb350ea56009ad52282c397d31b74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
